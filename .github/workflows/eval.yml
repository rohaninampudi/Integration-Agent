# Integration Agent Evaluation Workflow
# 
# This workflow runs the evaluation harness whenever prompts are changed.
# It tracks performance evolution across prompt iterations.

name: Run Evaluations

on:
  push:
    paths:
      - 'prompts/**'
      - 'src/**'
      - 'tools/**'
      - 'tests/eval_harness.py'
    branches:
      - main
      - 'feature/**'
  pull_request:
    paths:
      - 'prompts/**'
      - 'src/**'
      - 'tools/**'
      - 'tests/eval_harness.py'
  workflow_dispatch:
    inputs:
      use_mock:
        description: 'Use mock agent instead of real API'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.13'

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run unit tests
        run: |
          python -m pytest tests/ -v --tb=short --ignore=tests/eval_harness.py

  evaluate:
    name: Run Evaluation
    runs-on: ubuntu-latest
    needs: test
    outputs:
      eval_name: ${{ steps.git_info.outputs.eval_name }}
    
    # Only run with real agent if we have the API key
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for git SHA
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Get current git SHA and date
        id: git_info
        run: |
          echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "branch=${GITHUB_REF_NAME}" >> $GITHUB_OUTPUT
          echo "date=$(date +%m-%d-%Y)" >> $GITHUB_OUTPUT
          echo "eval_name=eval_$(date +%m-%d-%Y)_$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
      
      - name: Run evaluation (mock)
        if: ${{ github.event.inputs.use_mock == 'true' || env.OPENAI_API_KEY == '' }}
        run: |
          echo "Running with mock agent..."
          python tests/eval_harness.py --mock --output ${{ steps.git_info.outputs.eval_name }}.json
      
      - name: Run evaluation (real)
        if: ${{ github.event.inputs.use_mock != 'true' && env.OPENAI_API_KEY != '' }}
        run: |
          echo "Running with real agent..."
          python tests/eval_harness.py --real --output ${{ steps.git_info.outputs.eval_name }}.json
      
      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-${{ steps.git_info.outputs.eval_name }}
          path: results/eval_*.json
          retention-days: 30
      
      - name: Compare with baseline (if exists)
        continue-on-error: true
        run: |
          # Find the most recent baseline (excludes current run)
          BASELINE=$(ls -t results/eval_*.json 2>/dev/null | grep -v "${{ steps.git_info.outputs.eval_name }}" | head -1)
          CURRENT="results/${{ steps.git_info.outputs.eval_name }}.json"
          
          if [ -n "$BASELINE" ] && [ -f "$BASELINE" ] && [ -f "$CURRENT" ]; then
            echo "Comparing $BASELINE vs $CURRENT"
            python tests/eval_harness.py --compare "$BASELINE" "$CURRENT"
          else
            echo "No baseline found for comparison"
          fi
      
      - name: Generate summary
        env:
          RESULT_FILE: results/${{ steps.git_info.outputs.eval_name }}.json
          GIT_SHA: ${{ steps.git_info.outputs.sha_short }}
          EVAL_DATE: ${{ steps.git_info.outputs.date }}
        run: |
          if [ -f "$RESULT_FILE" ]; then
            echo "## ðŸ“Š Evaluation Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Date | $EVAL_DATE |" >> $GITHUB_STEP_SUMMARY
            echo "| Git SHA | $GIT_SHA |" >> $GITHUB_STEP_SUMMARY
            
            # Extract and display metrics using Python
            python3 scripts/generate_summary.py "$RESULT_FILE" >> $GITHUB_STEP_SUMMARY
          fi

  # Optional: Commit results back to repo (for tracking)
  commit-results:
    name: Commit Results
    runs-on: ubuntu-latest
    needs: evaluate
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    permissions:
      contents: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download evaluation results
        uses: actions/download-artifact@v4
        with:
          name: eval-results-${{ needs.evaluate.outputs.eval_name }}
          path: results/
        continue-on-error: true
      
      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          if [ -n "$(git status results/ --porcelain)" ]; then
            git add results/
            git commit -m "ðŸ“Š Add evaluation results for $(git rev-parse --short HEAD)"
            git push
          else
            echo "No new results to commit"
          fi
